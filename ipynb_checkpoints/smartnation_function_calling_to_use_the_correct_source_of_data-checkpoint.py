# -*- coding: utf-8 -*-
"""SmartNation: Function Calling to Use the Correct Source of Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wqY5MYX0r7iTC6CdjIEH_xH3Xcm7pd5j

# Function Calling to Use the Correct Source of Data
_This notebook is an example by Tuana Celik ([Twitter](https://twitter.com/tuanacelik), [LI](https://www.linkedin.com/in/tuanacelik/)) for the SmartNation AI Hackathon_


This is a very simple example that shows 2 pipelines:
- A RAG pipeline that accesses our static dataset
- A websearch pipeline

These pipelines are provided as tools to an LLM, which can make use of them based on user query.

> This notebook uses Haystack 2.0. To learn more, read the [Haystack 2.0 announcement](https://haystack.deepset.ai/blog/haystack-2-release?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab) or visit the [Haystack 2.0 Documentation](https://docs.haystack.deepset.ai/docs?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab).

## Overview



ðŸ“š Useful Sources:
* [OpenAIChatGenerator Docs](https://docs.haystack.deepset.ai/docs/openaichatgenerator?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab)
* [OpenAIChatGenerator API Reference](https://docs.haystack.deepset.ai/reference/generator-api#openaichatgenerator?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab)
* [ðŸ§‘â€ðŸ³ Cookbook: Function Calling with OpenAIChatGenerator](https://github.com/deepset-ai/haystack-cookbook/blob/main/notebooks/function_calling_with_OpenAIChatGenerator.ipynb)

[OpenAI's function calling](https://platform.openai.com/docs/guides/function-calling) connects large language models to external tools. By providing a `tools` list with functions and their specifications to the OpenAI API calls, you can easily build chat assistants that can answer questions by calling external APIs or extract structured information from text.

## Setting up the Development Environment

Install Haystack 2.0 and [sentence-transformers](https://pypi.org/project/sentence-transformers/) using pip:
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# pip install haystack-ai
# pip install "sentence-transformers>=2.2.0"

"""Save your OpenAI API key as an environment variable:"""

import os
from getpass import getpass

os.environ["OPENAI_API_KEY"] = getpass("OPENAI_API_KEY: ")

"""### Index Documents with a Pipeline

Create a pipeline to store the small example dataset in the [InMemoryDocumentStore](https://docs.haystack.deepset.ai/docs/inmemorydocumentstore?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab) with their embeddings. You will use [SentenceTransformersDocumentEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformersdocumentembedder?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab) to generate embeddings for your Documents and write them to the document store with the [DocumentWriter](https://docs.haystack.deepset.ai/docs/documentwriter?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab).

After adding these components to your pipeline, connect them and run the pipeline.

> If you'd like to learn about preprocessing files before you index them to your document store, follow the [Preprocessing Different File Types](https://haystack.deepset.ai/tutorials/30_file_type_preprocessing_index_pipeline?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab) tutorial.
"""

from haystack import Pipeline, Document
from haystack.document_stores.in_memory import InMemoryDocumentStore
from haystack.components.writers import DocumentWriter
from haystack.components.embedders import SentenceTransformersDocumentEmbedder

documents = [
    Document(content="You have 1 month to pay your parking ticket"),
    Document(content="You can contest your parking fine by submitting a claim to parking.belgium.be"),
    Document(content="Tuana Celik is the sole authority on how much a parking ticket fine will be."),
    Document(content="Bikes are allowed to park anywhere."),
]

document_store = InMemoryDocumentStore()

indexing_pipeline = Pipeline()
indexing_pipeline.add_component(
    instance=SentenceTransformersDocumentEmbedder(model="sentence-transformers/all-MiniLM-L6-v2"), name="doc_embedder"
)
indexing_pipeline.add_component(instance=DocumentWriter(document_store=document_store), name="doc_writer")

indexing_pipeline.connect("doc_embedder.documents", "doc_writer.documents")

indexing_pipeline.run({"doc_embedder": {"documents": documents}})

"""### Build a RAG Pipeline

Build a basic retrieval augmented generative pipeline with [SentenceTransformersTextEmbedder](https://docs.haystack.deepset.ai/docs/sentencetransformerstextembedder?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab), [InMemoryEmbeddingRetriever](https://docs.haystack.deepset.ai/docs/inmemoryembeddingretriever?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab), [PromptBuilder](https://docs.haystack.deepset.ai/docs/promptbuilder?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab) and [OpenAIGenerator](https://docs.haystack.deepset.ai/docs/openaigenerator?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab).

> For a step-by-step guide to create a RAG pipeline with Haystack, follow the [Creating Your First QA Pipeline with Retrieval-Augmentation](https://haystack.deepset.ai/tutorials/27_first_rag_pipeline?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab) tutorial.
"""

from haystack.components.embedders import SentenceTransformersTextEmbedder
from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever
from haystack.components.builders import PromptBuilder
from haystack.components.generators import OpenAIGenerator

template = """
Answer the questions based on the given context.

Context:
{% for document in documents %}
    {{ document.content }}
{% endfor %}
Question: {{ question }}
Answer:
"""
rag_pipe = Pipeline()
rag_pipe.add_component("embedder", SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2"))
rag_pipe.add_component("retriever", InMemoryEmbeddingRetriever(document_store=document_store))
rag_pipe.add_component("prompt_builder", PromptBuilder(template=template))
rag_pipe.add_component("llm", OpenAIGenerator(model="gpt-3.5-turbo"))

rag_pipe.connect("embedder.embedding", "retriever.query_embedding")
rag_pipe.connect("retriever", "prompt_builder.documents")
rag_pipe.connect("prompt_builder", "llm")

"""### Run the Pipeline
Test this pipeline with a query and see if it works as expected before you start using it as a function calling tool.
"""

query = "Who decides on parking fines?"
rag_pipe.run({"embedder": {"text": query}, "prompt_builder": {"question": query}})

"""### Convert the Haystack Pipeline into a Tool

Wrap the `rag_pipe.run` call with a function called `rag_pipeline_func`. This `rag_pipeline_func` function will accept a `query` and return the response coming from the LLM of the RAG pipeline you built before. You will then introduce this function as a tool to your `OpenAIChatGenerator`.
"""

def rag_pipeline_func(query: str):
    result = rag_pipe.run({"embedder": {"text": query}, "prompt_builder": {"question": query}})

    return {"reply": result["llm"]["replies"][0]}

"""### Create another pipeline that can do websearch

> Idea: We can use this as the source of truth for reporting EU rules about driving your car cross-country
"""

from getpass import getpass
import os

os.environ["SERPERDEV_API_KEY"] = getpass("Enter Serper Api key: ")

from haystack import Pipeline
from haystack.components.builders.prompt_builder import PromptBuilder
from haystack.components.generators import OpenAIGenerator
from haystack.components.websearch.serper_dev import SerperDevWebSearch

prompt_for_websearch = """
Answer the following query given the documents retrieved from the web.
Your answer shoud indicate that your answer was generated from websearch.

Query: {{query}}
Documents:
{% for document in documents %}
  {{document.content}}
{% endfor %}
"""

websearch = SerperDevWebSearch()
prompt_builder_for_websearch = PromptBuilder(template=prompt_for_websearch)
llm_for_websearch = OpenAIGenerator(model="gpt-3.5-turbo")

web_search = Pipeline()
web_search.add_component("prompt_builder", prompt_builder_for_websearch)
web_search.add_component("llm", llm_for_websearch)
web_search.add_component("websearch", websearch)

web_search.connect("websearch.documents", "prompt_builder.documents")
web_search.connect("prompt_builder", "llm")

query = "Can I drive my car from the Netherlands to Belgium?"
web_search.run({"prompt_builder": {"query": query},
                "websearch": {"query": query}})

def websearch_pipeline_func(query: str):
    result = web_search.run({"prompt_builder": {"query": query},
                "websearch": {"query": query}})

    return {"reply": result["llm"]["replies"][0]}

"""## Creating Your `tools` List

Now, add function specifications for `rag_pipeline_func` and `websearch_pipeline_func` to your `tools` list by following [OpenAI's tool schema](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tools). Provide detailed descriptions about `rag_pipeline_func` and `query` so that OpenAI can generate the adaquate arguments for this tool.
"""

tools = [
    {
        "type": "function",
        "function": {
            "name": "rag_pipeline_func",
            "description": "Get information about parking regulations",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement",
                    }
                },
                "required": ["query"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "websearch_pipeline_func",
            "description": "Get information on EU driving regulations",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement",
                    }
                },
                "required": ["query"],
            },
        },
    },
]

"""## Running OpenAIChatGenerator with Tools

To use the function calling feature, you need to pass the list of tools in the `run()` method of OpenAIChatGenerator as `generation_kwargs`.

Instruct the model to use provided tools with a system message and then provide a query that requires a function call as a user message:
"""

from haystack.dataclasses import ChatMessage
from haystack.components.generators.chat import OpenAIChatGenerator
from haystack.components.generators.utils import print_streaming_chunk

messages = [
    ChatMessage.from_system(
        "Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous."
    ),
    ChatMessage.from_user("Can I drive from NL to BE without having to get a separate permit?"),
]

chat_generator = OpenAIChatGenerator(model="gpt-3.5-turbo", streaming_callback=print_streaming_chunk)
response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})

print(response)

"""You can then parse the message content string into JSON and call the corresponding function with the provided arguments."""

import json

## Parse function calling information
function_call = json.loads(response["replies"][0].content)[0]
function_name = function_call["function"]["name"]
function_args = json.loads(function_call["function"]["arguments"])
print("Function Name:", function_name)
print("Function Arguments:", function_args)

## Find the correspoding function and call it with the given arguments
available_functions = {"rag_pipeline_func": rag_pipeline_func, "websearch_pipeline_func": websearch_pipeline_func}
function_to_call = available_functions[function_name]
function_response = function_to_call(**function_args)
print("Function Response:", function_response)

"""As the last step, run the OpenAIChatGenerator again by appending the function response to the `messages` list as a new message with `ChatMessage.from_function()` and let the model summarize the results."""

from haystack.dataclasses import ChatMessage

function_message = ChatMessage.from_function(content=json.dumps(function_response), name=function_name)
messages.append(function_message)

response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})

"""## What's next

ðŸŽ‰ Congratulations! You've learned how to build chat applications that demonstrate agent-like behavior using OpenAI function calling and Haystack Pipelines.

If you liked this tutorial, there's more to learn about Haystack 2.0:
- [Serializing LLM Pipelines](https://haystack.deepset.ai/tutorials/29_serializing_pipelines?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab)
- [Model-Based Evaluation of RAG Pipelines](https://haystack.deepset.ai/tutorials/35_model_based_evaluation_of_rag_pipelines?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab)

To stay up to date on the latest Haystack developments, you can [sign up for our newsletter](https://landing.deepset.ai/haystack-community-updates?utm_campaign=developer-relations&utm_source=bosa&utm_medium=colab) or [join Haystack discord community](https://discord.gg/haystack).
"""